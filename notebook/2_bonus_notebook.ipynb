{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82846698-e57e-4675-85b4-39961d7fd972",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Workshop: Onboarding & optimizing AI/ML workloads on AWS with Amazon S3 and EKS\n",
    "\n",
    "---\n",
    "\n",
    "<font size='5' color='blue'>**Bonus Notebook: Using Amazon S3 Connector for PyTorch**</font>\n",
    "\n",
    "---\n",
    "\n",
    "Throughout this workshop, you utilized Mountpoint for Amazon S3 as the primary file interface for data I/O in your ML workloads. This notebook introduces the [**Amazon S3 Connector for PyTorch**](https://github.com/awslabs/s3-connector-for-pytorch), another open-source alternative designed for efficient data I/O directly from/to Amazon S3, specifically tailored for PyTorch training workloads.\n",
    "\n",
    "While this notebook complements the broader discussion on efficient data I/O from the main workshop notebook, it is designed to stand alone. Its primary focus is to provide a practical demonstration of the efficient **dataloading** and **checkpointing** capabilities offered by the **S3 Connector for PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "> _‚ö†Ô∏è **NOTE:** Before using this notebook, you must have run sections 1, 3 and 5 from `1_main_notebook.ipynb`_\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Set everything up\n",
    "2. Introducing **Amazon S3 Connector for PyTorch**\n",
    "3. Benchmarking dataloading with **S3 Connector for PyTorch**\n",
    "4. Checkpointing with **S3 Connector for PyTorch**\n",
    "5. Summary of this notebook\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb854df-95c9-489f-85c1-2af834f372ce",
   "metadata": {},
   "source": [
    "# 1. Set everything up\n",
    "\n",
    "<a id='sec-1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009e6db-0b9f-47da-aecf-60bf459690f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font size='4' color='gray'>**_A few instructions before you start_**</font>\n",
    "\n",
    "<font size='3' color='green'>**_(1) Run each of the following code cells in turn with Shift+Enter._**</font>\n",
    "\n",
    "_It may take a few seconds to a few minutes for a code cell to run. You can determine whether a cell is running by examining the `[]:` indicator in the left margin next to each cell: a cell will show `[*]:` when running, and `[<a number>]:` when complete. **Please read on while you wait**._\n",
    "\n",
    "_Please feel free to review the code, but it is not essential for you to understand it as the important elements will be explained._\n",
    "\n",
    "<font size='3' color='red'>**_(2) If any cell output is in red, this indicates an error._**</font>\n",
    "\n",
    "_Check the  cells have been run in order, and seek help from a workshop assistant if needed._\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Install and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb909c5e-b5ef-4dfd-b16f-3c70cbdfbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import boto3                    # AWS SDK for Python\n",
    "import ray                      # Ray SDK for Python\n",
    "import ray.job_submission       # Ray Jobs interface module of Ray SDK for Python\n",
    "\n",
    "# Local imports\n",
    "from utilities import utils     # Collection of helper functions\n",
    "\n",
    "# Version check for Python compatibility\n",
    "MIN_PYTHON_VERSION = (3, 7)\n",
    "assert sys.version_info >= MIN_PYTHON_VERSION, f\"Python version must be {MIN_PYTHON_VERSION[0]}.{MIN_PYTHON_VERSION[1]} or higher.\"\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")\n",
    "print(f\"Ray SDK version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36150b81-3393-4081-8baf-96ee35c83cd1",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba2066-505b-4ac8-8dfd-95450add2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS region and S3 bucket configuration\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "s3_bucket_name = os.getenv(\"WORKSHOP_BUCKET\")\n",
    "s3_bucket_prefix = \"dataset\"\n",
    "\n",
    "# S3 bucket mountpoints\n",
    "local_mountpoint_dir = \"/s3_data\"       # S3 bucket mountpoint path on this Jupyter instance\n",
    "eks_mountpoint_dir = \"/mnt/s3_data\"     # S3 bucket mountpoint path on EKS cluster, as seen by Ray workers\n",
    "\n",
    "# Ray client configuration\n",
    "ray_head_dns = os.getenv(\"RAY_HEAD_NLB_DNS\")\n",
    "ray_head_port = 8265\n",
    "ray_address = f\"http://{ray_head_dns}:{ray_head_port}\"\n",
    "\n",
    "# Initialize Ray client\n",
    "ray_client = ray.job_submission.JobSubmissionClient(ray_address)\n",
    "\n",
    "# Print configurations\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "print(f\"S3 Bucket: {s3_bucket_name}\")\n",
    "print(f\"Ray Head DNS: {ray_client.get_address()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cbeaca7-bed8-489e-9c2c-a4a53d089bdf",
   "metadata": {},
   "source": [
    "# 2. Introducing **Amazon S3 Connector for PyTorch**\n",
    "\n",
    "The [**Amazon S3 Connector for PyTorch**](https://github.com/awslabs/s3-connector-for-pytorch) is an open-source toolset designed to provide high throughput for PyTorch training jobs that read from or write to Amazon S3. It streamlines the process by automatically optimizing performance when downloading training data or saving checkpoints, removing the need for lower-level custom code to handle tasks like listing S3 buckets or managing concurrent requests.\n",
    "\n",
    "The S3 Connector for PyTorch includes implementations of PyTorch's dataset primitives, enabling seamless loading of training data from Amazon S3. It supports both map-style datasets, suited for random data access patterns, and iterable-style datasets, ideal for streaming sequential data. Additionally, the connector offers a checkpointing interface that allows checkpoints to be saved and loaded directly from Amazon S3, bypassing the need for intermediate local storage.\n",
    "\n",
    "## 2.1 When to use **S3 Connector for PyTorch**\n",
    "\n",
    "Both **Mountpoint for S3** and the **S3 Connector for PyTorch** rely on the [**AWS Common Runtime**](https://aws.amazon.com/blogs/storage/improving-amazon-s3-throughput-for-the-aws-cli-and-boto3-with-the-aws-common-runtime/), which handles tasks such as automatic request parallelization, managing timeouts and retries, and reusing connections to optimize network performance and prevent overload. However, the two tools differ in their scope and use cases. Mountpoint for S3 serves as a general-purpose file interface to S3, supporting a wide range of workloads beyond ML training. In contrast, the S3 Connector for PyTorch provides a set of PyTorch-specific primitives designed to tightly integrate S3 with PyTorch training pipelines.\n",
    "\n",
    "One key distinction is in how each tool is used. When using S3 Connector for PyTorch, it is not required to install Mountpoint for S3 on your training nodes, which makes it great for training directly in notebooks or otherwise in environments where you can‚Äôt install software. The trade-off, however, is that using the S3 Connector for PyTorch requires minor modifications to your training code. These changes will be explained in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce2fb57-e478-44ae-a805-3a55d5361d04",
   "metadata": {},
   "source": [
    "# 3. Benchmarking dataloading with **S3 Connector for PyTorch**\n",
    "First we will benchmark the dataloading performance of the S3 Connector for PyTorch. We will submit the same `benchmark.py` training script used in the previous notebook (`1_main_notebook.ipynb`) as a remote Ray job. The benchmark parameters will remain identical to the earlier setup, but this time we will add a flag to use the S3 Connector for PyTorch for dataloading. Additionally, we will now need to provide the full S3 URI of the dataset in the `dataset_path` variable.\n",
    "\n",
    "> üí° _**TIP:** While the job runs, read on to learn more. It should take <font color='red'>**around 2 minutes**</font> to complete._\n",
    "\n",
    "\n",
    "## 3.1 Submit the job to Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bde7e1-d587-410b-8853-22ac78fcae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------\n",
    "### STEP #1: Compose the entrypoint command for Ray workers\n",
    "### -------\n",
    "\n",
    "# Set dataset name, dataset format, and benchmark name\n",
    "dataset_name = '100k-samples-large-files'\n",
    "dataset_format = 'tar'\n",
    "dataloader_use_s3pt = True\n",
    "dataset_path = os.path.join('s3://' + s3_bucket_name, s3_bucket_prefix, dataset_name)\n",
    "benchmark_name = f'benchmark-dataloading-{dataset_name}-{dataset_format}-{datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}'\n",
    "\n",
    "# Compose entrypoint command string\n",
    "entrypoint_command = \"python benchmark.py\" \\\n",
    "                     \"  --epochs=3\" \\\n",
    "                     \"  --batch_size=64\" \\\n",
    "                     \"  --prefetch_size=2\" \\\n",
    "                     \"  --input_dim=224\" \\\n",
    "                     \"  --dataloader_workers=16\" \\\n",
    "                    f\"  --dataloader_use_s3pt={dataloader_use_s3pt}\" \\\n",
    "                    f\"  --dataset_path={dataset_path}\" \\\n",
    "                    f\"  --dataset_format={dataset_format}\" \\\n",
    "                     \"  --model_compute_time=0\" \\\n",
    "                     \"  --ray_workers=2\" \\\n",
    "                     \"  --ray_cpus_per_worker=8\" \\\n",
    "                    f\"  --benchmark_name={benchmark_name}\"\n",
    "\n",
    "\n",
    "### --------\n",
    "### STEP #2: Define the runtime environment parameters for Ray workers\n",
    "### -------\n",
    "\n",
    "runtime_environment = {\n",
    "    \"working_dir\": \"./scripts\",        # <--- the working dir is copied over to each Ray worker\n",
    "    \"pip\": [                           # <--- PYPI packages to be installed on each Ray worker before executing entrypoint command\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'torchdata==0.9',\n",
    "        'webdataset',\n",
    "        's3torchconnector'],\n",
    "    \"env_vars\": {                      # <--- any custom env vars to be set in Ray worker runtime\n",
    "        'EKS_MOUNTPOINT_DIR': eks_mountpoint_dir,\n",
    "        'AWS_REGION': aws_region\n",
    "    }\n",
    "}\n",
    "\n",
    "### --------\n",
    "### STEP #3: Submit job to Ray cluster\n",
    "### -------\n",
    "\n",
    "job_id = ray_client.submit_job(entrypoint=entrypoint_command, runtime_env=runtime_environment)\n",
    "\n",
    "\n",
    "### Print out the Ray Job ID and other details\n",
    "print(f\"Submitted a new Ray job with ID '{job_id}' and the following entrypoint command: \\n\")\n",
    "for line in entrypoint_command.split('  '):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c21304-d19c-4c12-a829-21f5feabc0ac",
   "metadata": {},
   "source": [
    "## 3.2 Dataloading with **S3 Connector for PyTorch**\n",
    "\n",
    "The S3 Connector for PyTorch provides two abstractions for directly accessing data from S3 in your ML training scripts. For streaming scenarios, it includes a PyTorch **iterable-style dataset** that streams all objects from a specified prefix in your S3 bucket. This abstraction handles object listing and efficient loading internally, ensuring data is read only when necessary. With just a single line of code, you can construct a streaming dataset for the entire prefix of an S3 bucket. For random access needs, the connector offers a PyTorch **map-style dataset**. This dataset automatically lists and maps objects in a bucket prefix, allowing for direct access to specific objects.\n",
    "\n",
    "<img src=\"assets/pic_s3pt_dataloading.png\" width=\"1000\"/>\n",
    "\n",
    "Both data loaders are designed to be simple to use, **requiring only one line of code**, and they integrate seamlessly with PyTorch‚Äôs existing interfaces for data loading:\n",
    "\n",
    "- **Map-style datasets**:\n",
    "\n",
    "```python\n",
    "  from s3torchconnector import S3MapDataset\n",
    "\n",
    "  dataset = S3MapDataset.from_prefix(\"s3://reinvent-demo-bucket/dataset\")\n",
    "```\n",
    "\n",
    "- **Iterable-style datasets**:\n",
    "\n",
    "```python\n",
    "  from s3torchconnector import S3IterableDataset\n",
    "\n",
    "  dataset = S3IterableDataset.from_prefix(\"s3://reinvent-demo-bucket/dataset\")\n",
    "```\n",
    "\n",
    "\n",
    "## 3.3 Analyze and plot results (dataloading with S3 Connector for PyTorch)\n",
    "You can monitor the job in your Ray Dashboard and monitoring performance via the Grafana Dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50aab3-1374-4dd3-874e-9b6e931245fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grafana Dashboard: http://{os.getenv('GRAFANA_NLB_DNS')}\")\n",
    "print(f\"Ray Dashboard: http://{os.getenv('RAY_DASHBOARD_NLB_DNS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57fd8-139b-4582-99a3-f7e5e55ddf30",
   "metadata": {},
   "source": [
    "Let's now plot the results of the benchmark that we have just performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665e75b-a766-4003-bc64-a388afb405dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to finish, so that we can plot the results\n",
    "utils.wait_for_job_to_finish(job_id, ray_client)\n",
    "\n",
    "# Load the log file and plot the results\n",
    "logfile_path = os.path.join(local_mountpoint_dir, 'logs', benchmark_name + '.json')\n",
    "print(f\"Plotting results from '{logfile_path}'..\")\n",
    "utils.plot_dataloading_results(logfile_path, plot_for='s3pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197b3fe-edd3-4a24-99fa-436ce5133d7a",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The plot above illustrates the results of the data loading benchmark conducted using the S3 Connector for PyTorch with a sharded dataset. As demonstrated, streaming a sharded dataset directly from S3 achieves performance on par with our earlier benchmarks that utilized Mounpoint for S3. This approach enables efficient, on-demand dataset streaming directly from S3 into the PyTorch training script, eliminating the need for intermediate local storage."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "335c4f86-29da-4bdc-97b1-456a9e4ac5fc",
   "metadata": {},
   "source": [
    "# 4. Checkpointing with **S3 Connector for PyTorch**\n",
    "\n",
    "The S3 Connector for PyTorch also includes a checkpointing interface to save and load checkpoints directly to Amazon S3, without first saving to local storage. Similarly to Mountpoint for S3, it leverages [**AWS Common Runtime**](https://aws.amazon.com/blogs/storage/improving-amazon-s3-throughput-for-the-aws-cli-and-boto3-with-the-aws-common-runtime/) to distribute large file writes elastically across the S3 fleet, resulting in significantly faster model checkpointing performance than saving model snapshots to local EBS volumes or NVMe instance storage.\n",
    "\n",
    "The S3 Connector for PyTorch makes it also easy to save your PyTorch model checkpoints directly to S3. With just a single extra line of code, you can tell PyTorch to write a checkpoint directly to S3:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from s3torchconnector import S3Checkpoint\n",
    "\n",
    "ckpt = S3Checkpoint(region=\"us-west-2\")\n",
    "\n",
    "with ckpt.writer(\"s3://reinvent-demo-bucket/checkpoints/epoch-0.ckpt\") as writer:\n",
    "    torch.save(model.state_dict(), writer)\n",
    "```\n",
    "\n",
    "\n",
    "## 4.1 Benchmarking checkpointing with **S3 Connector for PyTorch**\n",
    "\n",
    "To quantitatively evaluate the checkpointing performance directly to S3 using **S3 Connector for PyTorch**, let's now run a comparative benchmark against PyTorch's native checkpointing capability to the local storage of Ray workers (which, in this case, are the attached EBS gp3 volumes). The benchmarks that we are about to run will utilize our previous benchmarking script, with a few additional configuration parameters to control checkpointing behavior:\n",
    "\n",
    "- `ckpt_steps` - defines number of steps between checkpoints (set to `100` in this benchmark, but setting to `0` will disable checkpointing);\n",
    "- `ckpt_mode` -  checkpointing backend, either `disk` (for checkpointing to local path), or `s3pt` (to use S3 Connector for PyTorch);\n",
    "- `ckpt_path` - storage path (S3 URI or local filesystem path);\n",
    "- `model_num_parameters` - model size in millions of parameters, which effectively defines the model snapshot size.\n",
    "\n",
    "> ‚ö†Ô∏è _**FEW NOTES:**_\n",
    "> - In this benchmark we are effectively using S3 Connector for PyTorch for **both** dataloading and checkpointing.\n",
    "> - As we set `epochs=1` and `ckpt_steps=100` below, we will checkpoint exactly **7 times** during our benchmark job and report the **average checkpointing time** (this is because each Ray worker processes ~750 batches per epoch, assuming 100k sample dataset, `batch_size=64` and `ray_workers=2`).\n",
    "> - As the we set `model_num_parameters=1000`, the resulting model snapshots will be approx. **4GB** in size. This is because we are saving 1000M weights in `fp32` format (i.e. allocating 4 bytes per model weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9ec5c-6d84-43a5-bee6-5d5e0c0e9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_benchmarks = {}\n",
    "\n",
    "for ckpt_destination in ('s3_connector', 'local_disk'):\n",
    "\n",
    "    ### --------\n",
    "    ### STEP #1: Compose the entrypoint command for Ray workers\n",
    "    ### -------\n",
    "\n",
    "    # Set dataset name, dataset format, and benchmark name\n",
    "    dataset_name = '100k-samples-large-files'\n",
    "    dataset_format = 'tar'\n",
    "    dataloader_use_s3pt = True\n",
    "    dataset_path = os.path.join('s3://' + s3_bucket_name, s3_bucket_prefix, dataset_name)\n",
    "    benchmark_name = f'benchmark-checkpointing-{ckpt_destination}-{datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}'\n",
    "\n",
    "    # Set checkpoint path and mode\n",
    "    if ckpt_destination == 's3_connector':\n",
    "        ckpt_mode = 's3pt'\n",
    "        ckpt_path = os.path.join('s3://' + s3_bucket_name, 'checkpoints')\n",
    "    else:\n",
    "        ckpt_mode = 'disk'\n",
    "        ckpt_path = 'checkpoints'\n",
    "    \n",
    "    # Compose entrypoint command string\n",
    "    entrypoint_command = \"python benchmark.py\" \\\n",
    "                         \"  --epochs=1\" \\\n",
    "                         \"  --batch_size=64\" \\\n",
    "                         \"  --prefetch_size=2\" \\\n",
    "                         \"  --input_dim=224\" \\\n",
    "                         \"  --dataloader_workers=4\" \\\n",
    "                        f\"  --dataloader_use_s3pt={dataloader_use_s3pt}\" \\\n",
    "                        f\"  --dataset_path={dataset_path}\" \\\n",
    "                        f\"  --dataset_format={dataset_format}\" \\\n",
    "                         \"  --model_compute_time=0\" \\\n",
    "                         \"  --ray_workers=2\" \\\n",
    "                         \"  --ray_cpus_per_worker=8\" \\\n",
    "                        f\"  --benchmark_name={benchmark_name}\" \\\n",
    "                         \"  --model_num_parameters=1000\" \\\n",
    "                         \"  --ckpt_steps=100\" \\\n",
    "                        f\"  --ckpt_mode={ckpt_mode}\" \\\n",
    "                        f\"  --ckpt_path={ckpt_path}\"\n",
    "    \n",
    "    \n",
    "    ### --------\n",
    "    ### STEP #2: Define the runtime environment parameters for Ray workers\n",
    "    ### -------\n",
    "    \n",
    "    # Nothing to do! We use the same runtime environment definition as for the first benchmark.\n",
    "    \n",
    "    ### --------\n",
    "    ### STEP #3: Submit job to Ray cluster\n",
    "    ### -------\n",
    "    \n",
    "    job_id = ray_client.submit_job(entrypoint=entrypoint_command, runtime_env=runtime_environment)\n",
    "    \n",
    "    \n",
    "    ### Print out the Ray Job ID and other details\n",
    "    print(f\"Submitted a new Ray job with ID '{job_id}' and the following entrypoint command: \\n\")\n",
    "    for line in entrypoint_command.split('  '):\n",
    "        print(line)\n",
    "    print('^'*40, '\\n')\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Keep track of our benchmarks\n",
    "    ckpt_benchmarks[job_id] = {'name': benchmark_name, 'tag': ckpt_destination}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40637b9d-6ab7-4096-9182-b341912f128f",
   "metadata": {},
   "source": [
    "### You can go to the Ray and Grafana dashboards to observe the TWO jobs running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a465e9-7b85-48e5-a947-1e8d1086237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grafana Dashboard: http://{os.getenv('GRAFANA_NLB_DNS')}\")\n",
    "print(f\"Ray Dashboard: http://{os.getenv('RAY_DASHBOARD_NLB_DNS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f9532-a905-4afc-b840-8ef63de87bb8",
   "metadata": {},
   "source": [
    "## 4.2 Analyze and plot results (checkpointing with S3 Connector for PyTorch)\n",
    "\n",
    "Now plot the results of your checkpointing benchmarks.\n",
    "\n",
    "> ‚ö†Ô∏è _**NOTE:** The **two** Ray jobs that we have just submnitted will run in parallel, and the longest job will take <font color='red'>**around 4 minutes**</font> to complete. The cell below will automatically wait for the job to complete, and then plot the benchmark results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74f6a5-22fd-450c-895d-b9d8f206ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to finish, so that we can plot the results\n",
    "for job_id in ckpt_benchmarks:\n",
    "    utils.wait_for_job_to_finish(job_id, ray_client)\n",
    "\n",
    "# Load the log files and plot the results\n",
    "benchmark_files = {\n",
    "    benchmark['tag'].replace('_', ' ').title(): os.path.join(local_mountpoint_dir, 'logs', benchmark['name'] + '.json')\n",
    "    for benchmark in ckpt_benchmarks.values()\n",
    "}\n",
    "\n",
    "print(f\"Plotting results for '{json.dumps(benchmark_files, indent=2)}'..\")\n",
    "utils.plot_checkpointing_results(benchmark_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa992963-b3a7-4379-b0e9-647ca1632c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Explanation\n",
    "\n",
    "The benchmark results above compare the time required to save periodic model checkpoints either directly to S3 using S3 Connector for PyTorch (red) or to a local EBS volume of the Ray workers using native PyTorch model checklpointing capability (blue). Similar to the benchmarks for Mountpoint for S3 in the previous notebook, the results here with S3 Connector for PyTorch demonstrate a clear advantage for saving checkpoints directly to S3.\n",
    "\n",
    "This high throughput, approaching the maximum network bandwidth of the EC2 instance, delivers substantial time and cost savings during training. This is particularly beneficial for long-running distributed training jobs with frequent checkpointing requirements, as it reduces overall training overhead and enables more efficient resource utilization.\n",
    "\n",
    ">üí° **_TIP:_** _In this workshop, we used EBS storage. If you use EC2 instances with instance storage, you'll also have access to one or more physically attached ephemeral volumes. Instance storage is ideal for temporary data such as buffers, caches, scratch data, or other transient content._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a63c1-65dd-4e3a-bd22-a6f6484ee13a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 5. Summary of this notebook\n",
    "\n",
    "This notebook introduced the **Amazon S3 Connector for PyTorch**, showcasing its capability for high-throughput dataset streaming directly from S3 and efficient model checkpointing back to S3. By offering PyTorch-specific primitives, the S3 Connector for PyTorch simplifies building efficient data I/O pipelines with seamless integration to Amazon S3. Benchmarks illustrated its potential to enhance ML workflows with minimal code changes, improving scalability and efficiency for real-world workloads.\n",
    "\n",
    "<br>\n",
    "\n",
    "- If you wish to share this workshop with colleagues, and/or run it in your own time in your own AWS Account, make a note of this link: https://s12d.com/stg406\n",
    "\n",
    "- If you *are* running this in your own AWS Account, in order to prevent ongoing charges you must clean up your deployed resources. Instructions for doing this are in the **Summary and clean-up section** of the workshop instructions.\n",
    "\n",
    "- If you are at an AWS event, **please fill out the session survey provided by AWS staff**. Your feedback helps us improve, and justifies our efforts in creating content such as this. Thank you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
