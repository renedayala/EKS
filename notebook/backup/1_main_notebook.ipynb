{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82846698-e57e-4675-85b4-39961d7fd972",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Workshop: Onboarding & optimizing AI/ML workloads on AWS with Amazon S3 and EKS\n",
    "\n",
    "---\n",
    "\n",
    "In the early phases of AI/ML project development, it’s common for individual contributors to work independently on laptops, personal servers, or cloud-based compute instances, each using their preferred IDE. A popular choice, and the one we’ll use in this workshop, is Jupyter Notebooks — a tool that offers an interactive environment for writing, testing, and refining code.\n",
    "\n",
    "In this notebook, we’ll begin by programmatically exploring the dataset stored on Amazon S3. With Mountpoint for S3, we can seamlessly use our standard Python libraries to interact with the dataset as if it were locally available on the instance. From there, we’ll transition from data exploration to launching distributed ML training on the same S3-hosted dataset. To achieve this, we’ll initiate multiple ML jobs using Ray on Amazon EKS, demonstrating how this setup supports _efficient data I/O_ for both **reading training datasets from S3** and **writing model checkpoints back to S3**.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Set everything up\n",
    "2. Explore the dataset\n",
    "3. Implement the benchmarking script\n",
    "4. Launch the first dataloading benchmark on the Ray cluster\n",
    "5. Shard the dataset\n",
    "6. Re-run the dataloding benchmark with sharded dataset\n",
    "7. Checkpoint models with Mountpoint for Amazon S3\n",
    "8. Summary and conclusions\n",
    "  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737006a-5585-4b62-b3b0-191c05e65c38",
   "metadata": {},
   "source": [
    "# 1. Set everything up\n",
    "\n",
    "<a id='sec-1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "<font size='4' color='gray'>**_A few instructions before you start_** - </font><font size='4' color='red'>**PLEASE READ THIS!**</font>\n",
    "\n",
    "<font size='3' color='green'>**_(1) Run each of the following code cells in turn with Shift+Enter._**</font>\n",
    "\n",
    "_It may take a few seconds to a few minutes for a code cell to run. You can determine whether a cell is running by examining the `[]:` indicator in the left margin next to each cell: a cell will show `[*]:` when running, and `[<a number>]:` when complete. **Please read on while you wait**._\n",
    "\n",
    "_Please feel free to review the code, but it is not essential for you to understand it as the important elements will be explained._\n",
    "\n",
    "<font size='3' color='red'>**_(2) If any cell output is in red, this indicates an error._**</font>\n",
    "\n",
    "_Check the  cells have been run in order, and seek help from a workshop assistant if needed._\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Install and import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb909c5e-b5ef-4dfd-b16f-3c70cbdfbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports\n",
    "import boto3                    # AWS SDK for Python\n",
    "import ray                      # Ray SDK for Python\n",
    "import ray.job_submission       # Ray Jobs interface module of Ray SDK for Python\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Local imports\n",
    "from utilities import utils     # Collection of helper functions\n",
    "\n",
    "# Version check for Python compatibility\n",
    "MIN_PYTHON_VERSION = (3, 7)\n",
    "assert sys.version_info >= MIN_PYTHON_VERSION, f\"Python version must be {MIN_PYTHON_VERSION[0]}.{MIN_PYTHON_VERSION[1]} or higher.\"\n",
    "\n",
    "# Print SDK versions\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Boto3 SDK version: {boto3.__version__}\")\n",
    "print(f\"Ray SDK version: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36150b81-3393-4081-8baf-96ee35c83cd1",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 Initial setup for clients and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba2066-505b-4ac8-8dfd-95450add2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS region and S3 bucket configuration\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "s3_bucket_name = os.getenv(\"WORKSHOP_BUCKET\")\n",
    "s3_bucket_prefix = \"dataset\"\n",
    "\n",
    "# S3 bucket mountpoints\n",
    "local_mountpoint_dir = \"/s3_data\"       # S3 bucket mountpoint path on this Jupyter instance\n",
    "eks_mountpoint_dir = \"/mnt/s3_data\"     # S3 bucket mountpoint path on EKS cluster, as seen by Ray workers\n",
    "\n",
    "# Ray client configuration\n",
    "ray_head_dns = os.getenv(\"RAY_HEAD_NLB_DNS\")\n",
    "ray_head_port = 8265\n",
    "ray_address = f\"http://{ray_head_dns}:{ray_head_port}\"\n",
    "\n",
    "# Initialize Ray client\n",
    "ray_client = ray.job_submission.JobSubmissionClient(ray_address)\n",
    "\n",
    "# Print configurations\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "print(f\"S3 Bucket: {s3_bucket_name}\")\n",
    "print(f\"Ray Head DNS: {ray_client.get_address()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f355ac-fa94-4ee7-a7aa-0f01de6d911a",
   "metadata": {},
   "source": [
    "# 2. Explore the dataset\n",
    "\n",
    "<a id='sec-2'></a>\n",
    "\n",
    "Let's start by exploring the dataset you uploaded to S3. As your S3 bucket is already mounted to this machine with **Mountpoint for Amazon S3**, you can interact with the dataset as if it were available locally. Later on, you will use the very same dataset on S3 to train your ML workloads on the Ray cluster.\n",
    "\n",
    "<img src=\"./assets/pic_s3_data_mountpoints.png\" width=\"1000\" align=\"center\"/>\n",
    "\n",
    "## 2.1 Print key statistics of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720003d-ae6d-4183-9db6-5c3effdd2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the local path for the dataset\n",
    "dataset_path = os.path.join(local_mountpoint_dir, s3_bucket_prefix, '100k-samples-small-files')\n",
    "print(\"Local path:\", dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820a05e-33fe-4b1e-a483-c5168b20ead5",
   "metadata": {},
   "source": [
    "The dataset is comprised of **100,000 JPG images**, where each file is a training sample stored in **4 different subfolders**. This dataset can be used to train an ML model for image classification. Therefore, the folder names also serve as the corresponding class labels of the images stored in them. Let's print what subfolders (or class labels) we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d32158-224f-4262-89c2-8323df24842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List subfolders under the dataset path, which are also the class names\n",
    "dataset_classes = os.listdir(dataset_path)\n",
    "dataset_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a9aea-d7ea-4983-b5b1-836f764b63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one of the classes in the dataset\n",
    "class_name = dataset_classes[0]\n",
    "\n",
    "# Print some file stats of a few random images from this class (or subfolder)\n",
    "class_path = os.path.join(dataset_path, class_name)\n",
    "selected_images = random.sample(os.listdir(class_path), 5)\n",
    "\n",
    "print(f\"A few random files of class '{class_name}':\")\n",
    "for image_name in selected_images:\n",
    "    image_path = os.path.join(class_path, image_name)\n",
    "    \n",
    "    # Get the size on disk (in KB)\n",
    "    size_on_disk_kb = os.path.getsize(image_path) / 1024\n",
    "    \n",
    "    # Open the image to get its dimensions\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        print(f\"  - {image_path}: {width}x{height} pixels, Size on disk: {size_on_disk_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10318ec9-7b78-4287-9538-92c9c10c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of files in each subfolder / class \n",
    "total_count = 0\n",
    "\n",
    "# List the images paths of each class\n",
    "for class_name in dataset_classes:\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    # Count the jpg images in the class directory\n",
    "    image_count = len(os.listdir(class_path))\n",
    "    total_count += image_count\n",
    "    print(f\"Class '{class_name}': {image_count} images\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Total: {total_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff9417-b765-4dd7-9ac0-84be1af1182f",
   "metadata": {},
   "source": [
    "## 2.2 Visualize a few training samples\n",
    "\n",
    "The synthetic images in this dataset were generated with Gaussian noise, where each class has distinct mean pixel values centered around 4 unique levels (one for each class). This approach is one of many techniques to rapidly create synthetic image datasets that are suitable for training machine learning algorithms for image classification. Let’s now display a few sample images from each class and overlay their average pixel values to verify the distinction between classes, as well as variation between samples withing each class:\n",
    "\n",
    "> 🕥 _The operation may take 15 seconds to complete._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24079c33-4c46-40d8-bc5d-6fc92ab9d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_images(dataset_path, num_images_per_class=5, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12b0a4-c5dd-4136-a4b5-f4f97718c011",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Implement the benchmarking script\n",
    "\n",
    "<a id='sec-3'></a>\n",
    "\n",
    "Execute the following cell to create the custom benchmarking script, which Ray will execute remotely as a distributed ML training job. <br>\n",
    "**We will discuss this script in more detail once the job is running.**\n",
    "\n",
    "**<font color='red'>The</font> `benchmark.py` <font color='red'>script</font>**:\n",
    "\n",
    "> ⚠️ **_NOTE:_**  The cell below creates the actual `benchmark.py` script that is going to be executed remotely by Ray on Amazon EKS. You are encouraged to study the code, but please **_DO NOT_** change any lines of code during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377f37a-7c05-4792-b9ca-7bc866eee6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/benchmark.py\n",
    "\n",
    "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "# SPDX-License-Identifier: MIT-0\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import webdataset as wds\n",
    "import s3torchconnector as s3pt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchdata\n",
    "from torchvision.transforms import v2 as tvt\n",
    "\n",
    "import ray.train\n",
    "import ray.train.torch\n",
    "\n",
    "\n",
    "################## BENCHMARK PARAMETERS DEFINITION ###################\n",
    "\n",
    "def parse_args():\n",
    "    \n",
    "    def none_or_int(value):\n",
    "        if str(value).upper() == 'NONE':\n",
    "            return None\n",
    "        return int(value)\n",
    "    \n",
    "    def none_or_str(value):\n",
    "        if str(value).upper() == 'NONE':\n",
    "            return None\n",
    "        return str(value)\n",
    "    \n",
    "    def str_bool(value):\n",
    "        if str(value).upper() == 'TRUE':\n",
    "            return True\n",
    "        elif str(value).upper() == 'FALSE':\n",
    "            return False\n",
    "        else:\n",
    "            raise TypeError(\"Must be True or False.\")\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ### Parameters that define dataloader config\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--dataloader_workers', type=int, default=0)\n",
    "    parser.add_argument('--dataloader_use_s3pt', type=str_bool, default=False)\n",
    "    parser.add_argument('--prefetch_size', type=none_or_int, default=2)\n",
    "    parser.add_argument('--input_dim', type=int, default=224)\n",
    "    parser.add_argument('--pin_memory', type=str_bool, default=True)\n",
    "\n",
    "    ### Parameters that define dataset config\n",
    "    parser.add_argument('--dataset_path', type=str)\n",
    "    parser.add_argument('--dataset_format', type=str)\n",
    "    parser.add_argument('--dataset_num_samples', type=int, default=100_000)\n",
    "    parser.add_argument('--dataset_region', type=none_or_str, default=os.getenv('AWS_REGION'))\n",
    "\n",
    "    ### Parameters that define model parameters\n",
    "    parser.add_argument('--model_compute_time', type=none_or_int, default=None) # in miliseconds\n",
    "    parser.add_argument('--model_num_parameters', type=int, default=1) # in millions of parameters\n",
    "    \n",
    "    ### Parameters that define benchmark infrastructure\n",
    "    parser.add_argument('--ray_workers', type=int, default=2)\n",
    "    parser.add_argument('--ray_cpus_per_worker', type=int, default=8)\n",
    "    parser.add_argument('--ray_use_gpu', type=str_bool, default=False)\n",
    "   \n",
    "    ### Parameters that define checkpointing config\n",
    "    parser.add_argument('--ckpt_steps', type=int, default=0)\n",
    "    parser.add_argument('--ckpt_mode', type=str, default='disk')\n",
    "    parser.add_argument('--ckpt_path', type=str, default='checkpoints/')\n",
    "    parser.add_argument('--ckpt_region', type=none_or_str, default=os.getenv('AWS_REGION'))\n",
    "\n",
    "    ### Some other parameters for logging results\n",
    "    parser.add_argument('--log_directory', type=none_or_str, default=os.path.join(os.getenv('EKS_MOUNTPOINT_DIR', '.'), 'logs'))\n",
    "    parser.add_argument('--benchmark_name', type=none_or_str, default=f'benchmark-{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}')\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "################## MODEL IMPLEMENTATION ###################\n",
    "\n",
    "class ModelMock(torch.nn.Module):\n",
    "    '''Model mock to emulate a computation of a training step'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Linear(config.model_num_parameters * 1_000_000, 1)\n",
    "        self.config = config\n",
    "    \n",
    "    def forward(self, data, target, epoch, step):\n",
    "        if self.config.model_compute_time > 0:\n",
    "            return time.sleep(self.config.model_compute_time / 1_000)\n",
    "\n",
    "        if (\n",
    "            ray.train.get_context().get_world_rank() == 0 and\n",
    "            self.config.ckpt_steps > 0 and\n",
    "            step % self.config.ckpt_steps == 0\n",
    "        ):\n",
    "            return self.save_checkpoint(epoch, step)\n",
    "\n",
    "    def save_checkpoint(self, epoch, step):\n",
    "        if self.config.ckpt_mode == 's3pt':\n",
    "            return save_checkpoint_s3pt(self.model, self.config.ckpt_region, self.config.ckpt_path, epoch, step)\n",
    "        elif self.config.ckpt_mode == 'disk':\n",
    "            return save_checkpoint_disk(self.model, self.config.ckpt_path, epoch, step)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown checkpoint mode '%s'..\" % self.config.ckpt_mode)\n",
    "\n",
    "\n",
    "def save_checkpoint_s3pt(model, region, uri, epoch_id, step_id):\n",
    "    path = os.path.join(uri, f\"epoch-{epoch_id}-step-{step_id}.ckpt\")\n",
    "    checkpoint = s3pt.S3Checkpoint(region=region)\n",
    "    start_time = time.perf_counter()\n",
    "    with checkpoint.writer(path) as writer:\n",
    "        torch.save(model.state_dict(), writer)\n",
    "    end_time = time.perf_counter()\n",
    "    save_time = end_time - start_time\n",
    "    print_from_rank(f\"Saving checkpoint to {uri} took {save_time} seconds..\")\n",
    "    return save_time\n",
    "\n",
    "def save_checkpoint_disk(model, uri, epoch_id, step_id):\n",
    "    if not os.path.exists(uri):\n",
    "        os.makedirs(uri)\n",
    "    path = os.path.join(uri, f\"epoch-{epoch_id}-step-{step_id}.ckpt\")\n",
    "    start_time = time.perf_counter()\n",
    "    torch.save(model.state_dict(), path)\n",
    "    end_time = time.perf_counter()\n",
    "    save_time = end_time - start_time\n",
    "    print_from_rank(f\"Saving checkpoint to {path} took {save_time} seconds..\")\n",
    "    return save_time\n",
    "            \n",
    "\n",
    "################## DATASET IMPLEMENTATIONS ###################\n",
    "\n",
    "class MapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files, transform):\n",
    "        self._files = np.array(files)\n",
    "        self._transform = transform\n",
    "   \n",
    "    @staticmethod\n",
    "    def _get_label(file):\n",
    "        return file.split(os.path.sep)[-2]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read(file):\n",
    "        return Image.open(file).convert('RGB')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file = self._files[idx]\n",
    "        sample = self._transform(self._read(file))\n",
    "        label = int(self._get_label(file))    # Labels in [0, MAX) range\n",
    "        return sample, label\n",
    "\n",
    "def _make_pt_dataset(config, transform):\n",
    "    # Create a dataset from individual image files\n",
    "    \n",
    "    files = glob.glob(config.dataset_path + '/**/*.jpg')\n",
    "    dataset = MapDataset(files, transform)\n",
    "    return dataset\n",
    "\n",
    "def _make_wds_dataset(config, transform):\n",
    "    # Create a WebDataset from tar files and apply transformations\n",
    "    \n",
    "    def _create_sample(sample):\n",
    "        label, img = sample['__key__'], sample['jpg']\n",
    "        img = transform(img)\n",
    "        label = int(label.split('/')[-2])\n",
    "        return img, label\n",
    "    \n",
    "    files = glob.glob(config.dataset_path + '/*.tar')        \n",
    "    dataset = wds.WebDataset(files, shardshuffle=True, resampled=True, nodesplitter=wds.split_by_node)\n",
    "    dataset = dataset.decode('pil')\n",
    "    dataset = dataset.map(_create_sample)\n",
    "    dataset = dataset.with_epoch(config.dataset_num_samples // (config.ray_workers * config.dataloader_workers))\n",
    "    return dataset\n",
    "\n",
    "def _make_s3pt_dataset(config, transform):\n",
    "\n",
    "    def _tar_to_tuple(s3object):\n",
    "        return s3object.key, torchdata.datapipes.utils.StreamWrapper(s3object)\n",
    "    \n",
    "    def _create_sample(item):\n",
    "        label, img = item\n",
    "        img = transform(Image.open(img).convert('RGB'))\n",
    "        label = int(label.split('/')[-2])\n",
    "        return img, label\n",
    "\n",
    "    dataset = s3pt.S3IterableDataset.from_prefix(config.dataset_path, region=config.dataset_region)\n",
    "    dataset = torchdata.datapipes.iter.IterableWrapper(dataset)\n",
    "    if config.dataloader_workers > 0:\n",
    "        dataset = dataset.sharding_filter()\n",
    "    dataset = dataset.map(_tar_to_tuple)\n",
    "    dataset = dataset.load_from_tar()\n",
    "    dataset = dataset.map(_create_sample)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "################## BENCHMARK IMPLEMENTATIONS #################\n",
    "def build_dataloader(config):\n",
    "    # Define image transformations and build the dataloader based on dataset format\n",
    "    transform = tvt.Compose([\n",
    "        tvt.ToImage(),\n",
    "        tvt.ToDtype(torch.uint8, scale=True),\n",
    "        tvt.RandomResizedCrop(size=(config.input_dim, config.input_dim), antialias=False), #antialias=True\n",
    "        tvt.ToDtype(torch.float32, scale=True),\n",
    "        tvt.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Build dataset\n",
    "    if config.dataset_format == 'jpg':\n",
    "        dataset = _make_pt_dataset(config, transform)\n",
    "    elif config.dataset_format == 'tar':\n",
    "        if config.dataloader_use_s3pt:\n",
    "            dataset = _make_s3pt_dataset(config, transform)\n",
    "        else:\n",
    "            dataset = _make_wds_dataset(config, transform)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown dataset format '%s'..\" % config.dataset_format)\n",
    "\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=config.dataloader_workers,\n",
    "        batch_size=config.batch_size,\n",
    "        prefetch_factor=config.prefetch_size,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    # Build a model or a model mock based on provided config\n",
    "    if config.model_compute_time is not None:\n",
    "        model = ModelMock(config)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Need to set compute time explicitely..\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, config):\n",
    "    # Train model and collect metrics\n",
    "    metrics = {}\n",
    "    img_tot_list, ep_times, ckpt_times = [], [], []\n",
    "    t_train_start = t_epoch_start = time.perf_counter()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        img_tot = 0\n",
    "        \n",
    "        for step, (images, labels) in enumerate(dataloader, 1):\n",
    "\n",
    "            # Perform a training step and optionally save checkpoint\n",
    "            batch_size = len(images)\n",
    "            img_tot += batch_size\n",
    "\n",
    "            result = model(images, labels, epoch, step)\n",
    "            \n",
    "            if result:\n",
    "                ckpt_times.append(result)\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print_from_rank(f\"Epoch = {epoch} | Step = {step}\")\n",
    "\n",
    "        # Record metrics for each epoch\n",
    "        img_tot_list.append(img_tot)\n",
    "        ep_times.append(time.perf_counter() - t_epoch_start)\n",
    "        t_epoch_start = time.perf_counter()\n",
    "\n",
    "    # Summarize training metrics\n",
    "    t_train_tot = time.perf_counter() - t_train_start\n",
    "    metrics['training_time'] = t_train_tot\n",
    "    metrics['samples_per_second'] = sum(img_tot_list) / t_train_tot\n",
    "    metrics['samples_processed_total'] = sum(img_tot_list)\n",
    "    metrics.update({f't_epoch_{i}': t for i, t in enumerate(ep_times, 1)})\n",
    "    metrics.update({f't_ckpt_{i}': t for i, t in enumerate(ckpt_times, 1)})\n",
    "    if ckpt_times:\n",
    "        metrics['t_ckpt_ave'] = sum(ckpt_times) / len(ckpt_times)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "################## HELPER FUNCTIONS #################\n",
    "def print_from_rank(msg, rank=0):\n",
    "    if ray.train.get_context().get_world_rank() == rank:\n",
    "        print(f'[r:{rank}]:', msg)\n",
    "\n",
    "\n",
    "############ MAIN EXECUTABLE FUNCTION ##############\n",
    "def main_fn(config):\n",
    "\n",
    "    # Print debugging information and configuration\n",
    "    print_from_rank(\"Benchmarking params:\\n\" + json.dumps(vars(config), indent=2))\n",
    "    print_from_rank(\"Environment variables:\\n\")\n",
    "    for k, v in os.environ.items():\n",
    "        print_from_rank(f'{k}={v}')\n",
    "\n",
    "    # # Print example dataset files for debugging\n",
    "    filelist_gen = glob.iglob(os.path.join(config.dataset_path, '**', '*'), recursive=True)\n",
    "    print_from_rank(f\"Files in {config.dataset_path}:\")\n",
    "    for i, f in enumerate(filelist_gen):\n",
    "        print_from_rank(\" - \" + f)\n",
    "        if i > 10: break\n",
    "    \n",
    "    # Step #1: Build dataloader and prepare it for Ray distributed environment\n",
    "    dataloader = build_dataloader(config)\n",
    "    dataloader = ray.train.torch.prepare_data_loader(dataloader)\n",
    "\n",
    "    # Step #2: Build model and prepare it for Ray distributed environment\n",
    "    model = build_model(config)\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    # Step #3: Train the model and collect metrics\n",
    "    metrics = train_model(model, dataloader, config)\n",
    "    \n",
    "    # Step #4: Log metrics and save to S3      \n",
    "    os.makedirs(config.log_directory, exist_ok=True)\n",
    "    log_file = os.path.join(config.log_directory, config.benchmark_name + '.json')\n",
    "    with open(log_file, 'w') as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    print_from_rank(f\"Logged the following metrics to '{log_file}':\\n\" + json.dumps(metrics, indent=2))\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    return\n",
    "        \n",
    "\n",
    "################## ENTRY POINT #################\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Parse configuration arguments\n",
    "    train_config, _ = parse_args()\n",
    "\n",
    "    # Set up scaling configuration for Ray Trainer\n",
    "    scaling_config = ray.train.ScalingConfig(\n",
    "        num_workers=train_config.ray_workers,\n",
    "        use_gpu=train_config.ray_use_gpu,\n",
    "        resources_per_worker={\n",
    "            'CPU': train_config.ray_cpus_per_worker\n",
    "        })\n",
    "    \n",
    "    # Initialize Ray TorchTrainer with main function\n",
    "    trainer = ray.train.torch.TorchTrainer(\n",
    "        main_fn,\n",
    "        scaling_config=scaling_config,\n",
    "        train_loop_config=train_config)\n",
    "\n",
    "    # Run the distributed training job\n",
    "    result = trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45785287-f415-44c1-9995-b2ac48a8c787",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. Launch the first dataloading benchmark on the Ray cluster\n",
    "\n",
    "<a id='sec-4'></a>\n",
    "\n",
    "_It is benchmark time!_ You are about to submit your first remote distributed ML training job to the Ray cluster running on Amazon EKS. Using Ray Jobs API is an easy and recommended way of submitting locally developed applications to the Ray cluster for remote execution. All that you need to do is to:\n",
    "1. compose an **entrypoint command** (like `python my_script.py`) that will be executed remotely on each Ray worker;\n",
    "2. define our **runtime environment**, which specifies runtime dependencies and requirements of our executable scripts;\n",
    "3. **submit the job** via Submit Job API to Ray cluster.\n",
    "\n",
    "To evaluate the dataloading performance of our distributed training job, you'll run our benchmarking script with several key configuration parameters that control the training behavior and resource utilization:\n",
    "\n",
    "- `epochs` - number of complete passes through the training dataset (set to `3` here);\n",
    "- `batch_size` - number of training samples processed in each iteration (set to `64`);\n",
    "- `prefetch_size` - number of batches to prefetch in the data loading pipeline (set to `2`);\n",
    "- `input_dim` - input image dimension for the model (set to `224`);\n",
    "- `dataloader_workers` - number of parallel data loading processes (set to `16`);\n",
    "- `dataset_path` - location of the training dataset in the mounted shared filesystem;\n",
    "- `dataset_format` - format of the input dataset (either `jpg` or `tar`);\n",
    "- `model_compute_time` - in ms, artificial delay to mimick GPU computation step (set to `0` here, meaning that we iterate through our dataset as fast as CPUs can do that);\n",
    "- `ray_workers` - number of Ray workers for distributed training (set to `2`);\n",
    "- `ray_cpus_per_worker` - CPU cores allocated to each Ray worker (set to `8`);\n",
    "- `benchmark_name` - name of the benchmark for tracking (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bde7e1-d587-410b-8853-22ac78fcae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------\n",
    "### STEP #1: Compose the entrypoint command for Ray workers\n",
    "### -------\n",
    "\n",
    "# Set dataset name, dataset format, and benchmark name\n",
    "dataset_name = '100k-samples-small-files'\n",
    "dataset_format = 'jpg'\n",
    "dataset_path = os.path.join(eks_mountpoint_dir, s3_bucket_prefix, dataset_name)\n",
    "benchmark_name = f'benchmark-dataloading-{dataset_name}-{dataset_format}-{datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}'\n",
    "\n",
    "# Compose entrypoint command string\n",
    "entrypoint_command = \"python benchmark.py\" \\\n",
    "                     \"  --epochs=3\" \\\n",
    "                     \"  --batch_size=64\" \\\n",
    "                     \"  --prefetch_size=2\" \\\n",
    "                     \"  --input_dim=224\" \\\n",
    "                     \"  --dataloader_workers=16\" \\\n",
    "                    f\"  --dataset_path={dataset_path}\" \\\n",
    "                    f\"  --dataset_format={dataset_format}\" \\\n",
    "                     \"  --model_compute_time=0\" \\\n",
    "                     \"  --ray_workers=2\" \\\n",
    "                     \"  --ray_cpus_per_worker=8\" \\\n",
    "                    f\"  --benchmark_name={benchmark_name}\"\n",
    "\n",
    "\n",
    "### --------\n",
    "### STEP #2: Define the runtime environment parameters for Ray workers\n",
    "### -------\n",
    "\n",
    "runtime_environment = {\n",
    "    \"working_dir\": \"./scripts\",        # <--- the working dir is copied over to each Ray worker\n",
    "    \"pip\": [                           # <--- PYPI packages to be installed on each Ray worker before executing entrypoint command\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'torchdata',\n",
    "        'webdataset',\n",
    "        's3torchconnector'],\n",
    "    \"env_vars\": {                      # <--- any custom env vars to be set in Ray worker runtime\n",
    "        'AWS_REGION': aws_region,\n",
    "        'EKS_MOUNTPOINT_DIR': eks_mountpoint_dir\n",
    "    }\n",
    "}\n",
    "\n",
    "### --------\n",
    "### STEP #3: Submit job to Ray cluster\n",
    "### -------\n",
    "\n",
    "job_id = ray_client.submit_job(entrypoint=entrypoint_command, runtime_env=runtime_environment)\n",
    "\n",
    "\n",
    "### Print out the Ray Job ID and other details\n",
    "print(f\"Submitted a new Ray job with ID '{job_id}' and the following entrypoint command: \\n\")\n",
    "for line in entrypoint_command.split('  '):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd14e1-2e7d-4da9-b196-478a5a918b9a",
   "metadata": {},
   "source": [
    "## 4.1 Understanding the benchmarking script\n",
    "\n",
    "<a id='sec-3'></a>\n",
    "\n",
    "While our custom benchmarking script is being executed remotely by Ray, let’s quickly recap the high-level setup of the benchmark runtime environment. We will also outline key principles for designing an efficient data I/O implementation for ML training, as these concepts will be essential for our benchmark script implementation.\n",
    "\n",
    "<img src=\"assets/pic_efficient_ml_training.png\" width=\"1200\"/>\n",
    "\n",
    "**An efficient dataloading pipeline for ML workloads**\n",
    "\n",
    "While CPU resources are relatively inexpensive and abundant, modern GPUs are relatively scarce and more expensive. Therefore, when designing an end-to-end dataloading pipeline for ML training, it is essential to avoid GPU starvation as much as possible.\n",
    "\n",
    "The training data input pipeline implemented here incorporates some best practices, to help us keep I/O bottlenecks associated with data ingestion as low as possible, thereby minimizing GPU idle times. We leverage several features of the native PyTorch dataloader, including **dataloader parallelization, batch prefetching, and buffering**, to asynchronously overlap preprocessing tasks on the CPU with training steps on the GPU.\n",
    "\n",
    "The **preprocessing steps** consist of JPEG decoding and image resizing to a tensor of dimensions 224x224x3 before batching the training examples into mini-batches of 64 samples. This minimum set of preprocessing operations enables us to construct a complete training pipeline while keeping CPU-bound preprocessing overhead to a minimum.\n",
    "\n",
    "Additionally, we will utilize the **caching capabilities** of _Mountpoint for Amazon S3_, which can automatically cache training data on local storage, accelerating repetitive read requests from the dataloader. This significantly speeds up our ML training process, as we will demonstrate later.\n",
    "\n",
    "**Multi-node distributed ML training with Ray on Amazon EKS**\n",
    "\n",
    "In order to properly setup our training script for a multi-node distributed training environment with Ray, we utilize several utility functions from the Ray Train SDK for PyTorch. These utilities help manage resources, synchronize model weights, and optimize data loading across multiple nodes, as outlined in the [Ray Guide for PyTorch](https://docs.ray.io/en/latest/train/getting-started-pytorch.html). By leveraging these Ray capabilities, we can efficiently scale our training workload across hundreds of CPU and GPU nodes.\n",
    "\n",
    "\n",
    "**❗ An important note**:\n",
    "\n",
    "- In this workshop we are **running benchmarks on a CPU-backed instance**, since we are primarily interested in the I/O behaviour of the ML training pipeline and do not have to be training an actual model on GPUs to perform the ML Storage benchmarks. \n",
    "- We follow the common ML storage benchmark procedure of mocking (aka. simulating) the model computation step by sleeping for a pre-defined amount of time on each model training step (see, e.g. [DLIO](https://github.com/argonne-lcf/dlio_benchmark) or [MLPerf for Storage](https://github.com/mlcommons/storage) I/O benchmarking toolsets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72aa1aa-fc25-4be0-865f-20f168219154",
   "metadata": {},
   "source": [
    "## 4.2 Monitoring and observability for Ray jobs\n",
    "\n",
    "The Ray job you just submitted will take <font color='red'>**up to 10 minutes**</font> to complete. Later, you will learn how to improve performance with this dataset.\n",
    "\n",
    "While the job is running, you will connect to the Ray Dashboard, then the Grafana Dashboard, to monitor its progress.\n",
    "\n",
    "### 4.2.1 Ray Dashboard\n",
    "\n",
    "- Run the cell below to generate a link to the Ray Dashboard.\n",
    "- Follow the link to open the dashboard in a new browser tab.\n",
    "- On the Ray Dashboard, Choose the **Jobs** tab to track the job's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faffd5f-e532-4c52-a949-77923fe7b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ray Dashboard: http://{os.getenv('RAY_DASHBOARD_NLB_DNS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25222f80-2db1-4859-8fe4-518040668727",
   "metadata": {},
   "source": [
    "Here’s what the **Jobs** tab looks like in the Ray Dashboard, where will see any Ray job that is running in your cluster:\n",
    "\n",
    "<img src=\"assets/pic_ray_jobs.png\" width=\"900\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Each job starts in the **PENDING** state, as shown here:\n",
    "\n",
    "<img src=\"assets/pic_ray_pending.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "As this is your first Ray job, it will take about two minutes to bootstrap the Ray environment (subsequent jobs will have much lower startup overhead time in _pending_ state), it will transition to the **RUNNING** state:\n",
    "\n",
    "<img src=\"assets/pic_ray_running.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "You can choose the **Job ID** to access the **Logs** window, where you can view detailed job progress and any output generated by the job. As the job will run through multiple epochs on the dataset, you can also monitor the progress in the job logs:\n",
    "\n",
    "<img src=\"assets/pic_ray_logs.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "> 💡 **_TIP:_** Use the **Refresh** button to keep the logs printouts updated\n",
    "\n",
    "<br>\n",
    "\n",
    "Additionally, the **Cluster** tab in the Ray Dashboard provides information on worker node utilization.\n",
    "\n",
    "<img src=\"assets/pic_ray_cluster.png\" width=\"900\" align=\"center\"/>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d5d722-09b3-4db6-ad4d-53fa2ca85107",
   "metadata": {},
   "source": [
    "### 4.2.2 Grafana Monitoring Dashboard\n",
    "\n",
    "The Grafana dashboard is deployed as a Pod in our EKS cluster. It is worthwhile mentioning, that Grafana can also be embedded into the Ray Dashboard directly, but due the limitations of the enbedded Grafana dashboard, it is deployed _**separarately**_ for this workshop.\n",
    "\n",
    "- Run the cell below to generate a link to the Grafana Dashboard.\n",
    "- Follow the link to open the dashboard in a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4eb094-887a-41b6-b9bb-dc4483b22c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grafana Dashboard: http://{os.getenv('GRAFANA_NLB_DNS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6671ea86-b212-4ca7-b543-9881d59464bb",
   "metadata": {},
   "source": [
    "When prompted for credentials, enter the username: **_admin_** and password: **_admin_**.\n",
    "\n",
    "> ⚠️ _**NOTE:**  You will be prompted to select a new password. We suggest to choose **Skip** (under the **Submit** button), for simplicity._\n",
    "\n",
    "<br>\n",
    "<img src=\"assets/pic_grafana_open.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "\n",
    "Once logged in, select **Dashboards** on the left navigation pane, then choose the **Workshop** dashboard, which should be the only dashboard.\n",
    "\n",
    "<img src=\"assets/pic_grafana_dashboard.png\" width=\"600\" align=\"center\"/>\n",
    "\n",
    "You will than be able to see statistics in 5 second intervals, and monitor various task metrics of your Ray environment.\n",
    "\n",
    "<img src=\"assets/pic_grafana_dashboard2.png\" width=\"800\" align=\"center\"/>\n",
    "\n",
    "When you're done exploring, return to the Ray Dashboard to validate the job has reached the **SUCCEEDED** state:\n",
    "\n",
    "<img src=\"assets/pic_ray_succeeded.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "You can also run the cell in 4.3, below, as it will automatically wait until the Ray job completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671da8f-3361-4ba1-9676-a406a93c7b05",
   "metadata": {},
   "source": [
    "## 4.3 Analyze and plot results (dataloading with small files dataset)\n",
    "\n",
    "Let's now plot the results of the benchmark that we have just performed. Since the benchmark script has created a logfile in our S3 bucket for persistence, we can load it here from our locally mounted S3 bucket and plot it with the a helper function:\n",
    "\n",
    "> ⚠️ _**NOTE**: The Ray job you launched takes <font color='red'>**around 10 mins**</font> to complete. The cell below will automatically wait for the job to complete, and then plot the benchmark results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665e75b-a766-4003-bc64-a388afb405dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to finish, so that we can plot the results\n",
    "utils.wait_for_job_to_finish(job_id, ray_client)\n",
    "\n",
    "# Load the log file and plot the results\n",
    "logfile_path = os.path.join(local_mountpoint_dir, 'logs', benchmark_name + '.json')\n",
    "print(f\"Plotting results from '{logfile_path}'..\")\n",
    "utils.plot_dataloading_results(logfile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197b3fe-edd3-4a24-99fa-436ce5133d7a",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "The benchmark results displayed in the plot above illustrate the time per epoch on our Ray cluster nodes.\n",
    "\n",
    "The red line represents actual epoch times with data caching enabled. The first epoch takes considerably longer than subsequent ones, while the training script streams the dataset directly from S3. By the second epoch, the dataset has been fully cached on the local storage of Ray worker instances. Consequently, the data is read from the local instance storage, significantly reducing file access times and decreasing epoch durations. \n",
    "\n",
    "Streaming this dataset from S3 is relatively slow, due to it being comprised of a large number of small files and the high latency characteristic of S3 general purpose storage (see discussion in the next section).\n",
    "\n",
    "To understand the impact of caching, the graph compares this with a hypothetical scenario where the dataset is streamed from S3 for each epoch, without any local caching. Although this scenario wasn’t benchmarked directly, we can estimate that each epoch would take approximately as long as the first epoch of our benchmark with enabled caching. This hypothetical scenario is depicted by the blue dashed line in the plot.\n",
    "\n",
    "These results highlight the substantial efficiency gains from caching datasets locally, particularly when working with large collections of small files that can fit within the local storage capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf589e8-3dbe-40f7-bc0e-746a3c8b40ff",
   "metadata": {},
   "source": [
    "# 5. Shard the dataset\n",
    "\n",
    "<a id='sec-5'></a>\n",
    "\n",
    "In this section, we re-package our training dataset consistent of lots of small JPG files into a sharded dataset comprising of a few larger TAR files. While the task is running, we will discuss why sharding is helpful.\n",
    "\n",
    "## 5.1 Sharding our dataset with **Amazon S3 Tar Tool**\n",
    "Among various methods available for dataset sharding into TAR format, we leverage the **[Amazon S3 Tar Tool](https://github.com/awslabs/amazon-s3-tar-tool)** for this workshop. This open-source utility provides an efficient and streamlined approach to creating tarballs from existing Amazon S3 objects and storing them back to S3 through a single CLI command. \n",
    "\n",
    "> ⚠️ _**NOTE:**  We have pre-compiled an s3tar binary to save time in this workshop. In a production environment, you should install [make](https://www.gnu.org/software/make/), [go](https://go.dev/), and compile it from the latest build on the [**Amazon S3 Tar Tool** GitHub repository](https://github.com/awslabs/amazon-s3-tar-tool). See [detailed instructions here](https://github.com/awslabs/amazon-s3-tar-tool?tab=readme-ov-file#installation)._\n",
    "\n",
    "In what follows, we'll execute a sharding operation on our dataset comprised of **100,000 JPG files**, each approximately **100KB** in size. The process will concatinate these files into roughly **40 TAR archives**, with each archive approximately **250MB** in size and containing approximately 2,500 training samples. The entire operation is expected to complete in <font color='red'>**around 4 mins**</font>, outputting a sharded dataset in your S3 bucket. \n",
    "\n",
    "> 💡 _**TIP:**  While you wait for the next cell to complete, please read through _**Section 5.2**_ to learn why sharding can be a good idea when streaming datasets directly from S3 for ML training._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273fc43-d82f-4405-9321-9e4fc3a7da42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!time ./s3tar \\\n",
    "--size-limit 260000000 \\\n",
    "--region {aws_region} \\\n",
    "--concat-in-memory \\\n",
    "--storage-class STANDARD \\\n",
    "--goroutines 1000 \\\n",
    "-cvf s3://{s3_bucket_name}/{s3_bucket_prefix}/100k-samples-large-files/shard.tar \\\n",
    "s3://{s3_bucket_name}/{s3_bucket_prefix}/100k-samples-small-files/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6624e8-897f-45ca-aa26-8fdd9b58e4c4",
   "metadata": {},
   "source": [
    "## 5.2 The impact of data sharding on ML training efficiency\n",
    "\n",
    "When training machine learning models with data stored in S3, the way you organize your dataset can dramatically impact your training performance. Let's explore why sharded datasets offer superior throughput performance than datasets comprised of lots of small files and thus can help you to alleviate any potential I/O bottlenecks in your ML training pipelines.\n",
    "\n",
    "Let's take our computer vision dataset as an example, and consider two approaches to storing it in S3:\n",
    "\n",
    "1. **Individual files dataset** (one sample per file): Storing each sample as a separate 100KB JPG file\n",
    "2. **Sharded dataset** (multiple samples per file): Combining multiple image samples into larger 250MB shards\n",
    "\n",
    "\n",
    "<img src=\"assets/pic_sequential_vs_random.png\" width=\"1200\"/>\n",
    "\n",
    "In the non-sharded approach, individual training samples (100KB JPG files) are stored as separate objects in S3 storage. While this approach is straightforward, each sample retrieval requries a separate GET request to S3, incurring a Time-To-First-Byte (TTFB) latency which is typically ~50-200ms for S3 general purpose buckets. As can be seen on the illustration above, reading lots of small files from S3 results in significant cumulative overhead due to TTFB latencies and can lead to suboptimal throughput.\n",
    "> 💡 _**TIP**: The **S3 Express One Zone** storage class and the associated [S3 directory buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-one-zone.html) are designed for workloads or performance-critical applications that require consistent single-digit millisecond latency. While exploring this option is outside of the scope for this workshop, consider this bucket type when you require the lowest latency object storage in the cloud._\n",
    "\n",
    "Conversely, sharding our dataset by aggregating multiple training samples into consolidated objects (such as TAR-files), and then reading the training sample sequentially one after another from the data shards, we drastically reduce the number of GET requests to S3 in order to consume the dataset. This amortizes TTFB latency across multiple samples, substantially increasing the effective throughput.\n",
    "\n",
    "This makes sharding particularly crucial for ML training workflows, where you need to process thousands or millions of samples efficiently. Sharded data enables higher throughput, which can correspond to higher utilization of GPU resourecs.\n",
    "\n",
    "## 5.3 Inspect the sharded dataset\n",
    "Let's quickly inspect the sharded dataset that you have just created on S3. As you have already mounted your S3 bucket locally, you can use regular Linux commands as if the dataset were locally available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7aaba-1698-4b68-9e6c-efb9b35eebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -ha {local_mountpoint_dir}/{s3_bucket_prefix}/100k-samples-large-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4255021e-379f-42b1-baeb-5f7697f60a71",
   "metadata": {},
   "source": [
    "# 6. Re-run the dataloading benchmark with sharded dataset\n",
    "\n",
    "<a id='sec-6'></a>\n",
    "\n",
    "You will now compare performance between the sharded and original datasets, by executing the ML training job on the Ray cluster using your newly sharded dataset. To maintain experimental consistency, all benchmarking parameters remain identical to the previous run, with the only changes being the `dataset_path` and `dataset_format` parameters to accommodate the sharded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eb988-e9b4-4007-9408-757c4d0e7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --------\n",
    "### STEP #1: Compose the entrypoint command for Ray workers\n",
    "### -------\n",
    "\n",
    "# Set dataset name and format that we want to use in benchmark\n",
    "dataset_name = '100k-samples-large-files'\n",
    "dataset_format = 'tar'\n",
    "dataset_path = os.path.join(eks_mountpoint_dir, s3_bucket_prefix, dataset_name)\n",
    "benchmark_name = f'benchmark-dataloading-{dataset_name}-{dataset_format}-{datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}'\n",
    "\n",
    "# Compose entrypoint command string\n",
    "entrypoint_command = \"python benchmark.py\" \\\n",
    "                     \"  --epochs=3\" \\\n",
    "                     \"  --batch_size=64\" \\\n",
    "                     \"  --prefetch_size=2\" \\\n",
    "                     \"  --input_dim=224\" \\\n",
    "                     \"  --dataloader_workers=16\" \\\n",
    "                    f\"  --dataset_path={dataset_path}\" \\\n",
    "                    f\"  --dataset_format={dataset_format}\" \\\n",
    "                     \"  --model_compute_time=0\" \\\n",
    "                     \"  --ray_workers=2\" \\\n",
    "                     \"  --ray_cpus_per_worker=8\" \\\n",
    "                    f\"  --benchmark_name={benchmark_name}\"\n",
    "\n",
    "\n",
    "### --------\n",
    "### STEP #2: Define the runtime environment parameters for Ray workers\n",
    "### -------\n",
    "\n",
    "# Nothing to do! We use the same runtime environment definition as for the first benchmark.\n",
    "\n",
    "### --------\n",
    "### STEP #3: Submit job to Ray cluster\n",
    "### -------\n",
    "\n",
    "job_id = ray_client.submit_job(entrypoint=entrypoint_command, runtime_env=runtime_environment)\n",
    "\n",
    "\n",
    "### Print out the Ray Job ID and other details\n",
    "print(f\"Submitted a new Ray job with ID '{job_id}' and the following entrypoint command: \\n\")\n",
    "for line in entrypoint_command.split('  '):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fcbd6-c218-49a8-8697-959d280680d2",
   "metadata": {},
   "source": [
    "## 6.1 Analyze and plot results (dataloading with sharded dataset)\n",
    "\n",
    "Now plot the results of our second benchmark with sharded dataset.\n",
    "\n",
    "> ⚠️ _**NOTE:** The Ray job that you have just launched will take <font color='red'>**around 2 minutes**</font> to complete. The cell below will automatically wait for the job completion and plot the benchmark results. **Feel free to return to your Ray and Grafana dashboard browser tabs to monitor job progress.**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b995aa00-4614-4707-ba83-6964116a9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links to the Ray and Grafana dashboards\n",
    "print(f\"Ray Dashboard: http://{os.getenv('RAY_DASHBOARD_NLB_DNS')}\")\n",
    "print(f\"Grafana Dashboard: http://{os.getenv('GRAFANA_NLB_DNS')}\")\n",
    "print('-' * 60)\n",
    "\n",
    "# Wait for the job to finish, so that we can plot the results\n",
    "utils.wait_for_job_to_finish(job_id, ray_client)\n",
    "\n",
    "# Load the log file and plot the results\n",
    "logfile_path = os.path.join(local_mountpoint_dir, 'logs', benchmark_name + '.json')\n",
    "print(f\"Plotting results from '{logfile_path}'..\")\n",
    "utils.plot_dataloading_results(logfile_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d043c62-e5b2-46a5-b087-2dabd5a44d22",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "In this benchmark, you analyzed the impact of caching on training performance when using a sharded dataset. The red line shows actual epoch times with caching enabled, while the blue line represents a hypothetical scenario where data is streamed from S3 at each epoch without caching. The effect of caching on epoch time is significantly reduced (if not negligible), compared with the previous benchmark.\n",
    "\n",
    "This improvement is due to the advantages of streaming sharded data from S3, which we have discussed in the previous section. Looking at the red line, even the first epoch — where data is streamed directly from S3 before any caching has occurred — experiences minimal I/O bottleneck (as it is just as fast as the subsequent epochs). This indicates that sharding has effectively addressed the latency issues seen with the non-sharded dataset, achieving near-optimal data transfer rates right from the training outset.\n",
    "\n",
    "In summary, with sharding, there is little difference in performance between cached and uncached scenarios, as the S3 streaming throughput is sufficient to saturate the available compute resources in both cases. This finding suggests that while dataset caching remains an effective strategy for maximizing data pipeline throughput, sharding can also be a powerful tool for large datasets, especially those that exceed the storage capacity of local instances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ebe9cc8-4f68-474d-a376-b0185e358902",
   "metadata": {},
   "source": [
    "# 7. Checkpoint models with **Mountpoint for Amazon S3**\n",
    "\n",
    "<a id='sec-7'></a>\n",
    "\n",
    "Up to now we were only concerned with topic of efficient dataloading for ML training, which is essentially about how to _**get training data from S3 into the ML training cluster**_. Let us now turn our attention to the topic of model checkpointing, and see how we can _**get the model data out from the ML training cluster to S3**_ in the most efficient manner.\n",
    "\n",
    "## 7.1 The critical role of model checkpointing in large-scale training\n",
    "\n",
    "Model checkpointing is a fundamental mechanism in machine learning workflows that periodically saves the complete training state, including model weights, optimizer states, and other training parameters. In large-scale distributed training environments, where computations run across hundreds or thousands of nodes for days or weeks, checkpointing becomes crucial for fault tolerance and experiment reproducibility. Without efficient checkpointing, a single node failure could result in the loss of days of training progress, necessitating a complete restart. However, traditional checkpointing mechanisms often create a significant performance bottleneck, as when saving state, all nodes must (typically) pause their training until the checkpoint operation completes, directly impacting training time and resource utilization. This challenge is particularly acute in modern deep learning models with billions of parameters, where checkpoint sizes can reach hundreds of gigabytes. Additionally, one needs to implement auxilliary background syncing mechanisms between local storage and Amazon S3 for persistently storing model states. Therefore, an efficient checkpointing solution that minimizes training interruption, while ensuring reliable state preservation, is essential for production-scale machine learning operations.\n",
    "\n",
    "<img src=\"assets/pic_ckpting_to_local_vs_s3_storage.png\" width=\"1200\"/>\n",
    "\n",
    "\n",
    "## 7.2 Using **Mountpoint for S3** for high-performance model checkpointing\n",
    "\n",
    "[**Mountpoint for Amazon S3**](https://github.com/awslabs/mountpoint-s3) streamlines ML checkpointing directly to Amazon S3. This eliminates the traditional two-step process of first saving model snapshots to local storage and then uploading (or syncing) them to cloud storage for persistence, which adds to both I/O overhead and operational complexity. Mountpoint for S3 leverages [**AWS Common Runtime**](https://aws.amazon.com/blogs/storage/improving-amazon-s3-throughput-for-the-aws-cli-and-boto3-with-the-aws-common-runtime/) to distribute large file writes elastically across the S3 fleet, resulting in up to 60% faster model checkpointing performance than saving model snapshots to local NVMe instance storage. This superior performance is possible because, instead of being bottlenecked by the bandwidth of a single local disk, Mountpoint for Amazon S3 can parallelize the object uploads across the Amazon S3 fleet, and burst to hundreds of gigabits per second during the checkpointing process.\n",
    "\n",
    "<img src=\"assets/pic_ckpting_with_crt.png\" width=\"1200\"/>\n",
    "\n",
    "Beyond performance, the low cost of Amazon S3, particularly when using the [Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/) storage class, make it ideal for storing your model checkpoints for the long term. And you can quickly and easily return to earlier experiments. By leveraging Mountpoint for S3, no code changes are required to your training scripts.\n",
    "\n",
    "## 7.3 Running model checkpointing benchmarks on Ray cluster\n",
    "\n",
    "\n",
    "_It's benchmarking time again!_ To quantitatively evaluate the checkpointing performance directly to S3 using **Mountpoint for Amazon S3**, let's now run a comparative benchmark against checkpointing to local storage of the Ray workers (which is the attached EBS gp3 volume in our case). The benchmarks that we are about to run will utilize our previous benchmarking script, with a few additional configuration parameters to control checkpointing behavior:\n",
    "\n",
    "- `ckpt_steps` - defines number of steps between checkpoints (set to `100` in this benchmark, but setting to `0` will disable checkpointing);\n",
    "- `ckpt_mode` -  checkpointing backend, either `disk` (for checkpointing to local path), or `s3pt` (to use S3 Connector for PyTorch);\n",
    "- `ckpt_path` - storage path (S3 URI or local filesystem path);\n",
    "- `model_num_parameters` - model size in millions of parameters, which effectively defines the model snapshot size.\n",
    "\n",
    "> ⚠️ _**FEW NOTES:**_\n",
    "> - _As the we set `epochs=1` and `ckpt_steps=100` below, we will checkpoint exactly 7 times during our benchmark job and report the **average checkpointing time** (this is because each Ray worker processes ~750 batches per epoch, assuming 100k sample dataset, `batch_size=64` and `ray_workers=2`);_\n",
    "> - _As the we set `model_num_parameters=1000`, the resulting model snapshots will be approx. **4GB** in size. This is because we are saving 1000M weights in `fp32` format (i.e. allocating 4 bytes per model weight)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb9ec5c-6d84-43a5-bee6-5d5e0c0e9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_benchmarks = {}\n",
    "\n",
    "for ckpt_destination in ('mountpoint', 'local_disk'):\n",
    "\n",
    "    ### --------\n",
    "    ### STEP #1: Compose the entrypoint command for Ray workers\n",
    "    ### -------\n",
    "\n",
    "    # Set dataset name, dataset format, and benchmark name\n",
    "    dataset_name = '100k-samples-large-files'\n",
    "    dataset_format = 'tar'\n",
    "    dataset_path = os.path.join(eks_mountpoint_dir, s3_bucket_prefix, dataset_name)\n",
    "    benchmark_name = f'benchmark-checkpointing-{ckpt_destination}-{datetime.now().strftime(\"%Y%m%d%H%M%S-%f\")}'\n",
    "\n",
    "    # Set checkpoint path on Ray cluster: either S3 mount point path, or local volume path\n",
    "    ckpt_path = os.path.join(eks_mountpoint_dir, 'checkpoints') if ckpt_destination == 'mountpoint' else 'checkpoints/'\n",
    "    \n",
    "    # Compose entrypoint command string\n",
    "    entrypoint_command = \"python benchmark.py\" \\\n",
    "                         \"  --epochs=1\" \\\n",
    "                         \"  --batch_size=64\" \\\n",
    "                         \"  --prefetch_size=2\" \\\n",
    "                         \"  --input_dim=224\" \\\n",
    "                         \"  --dataloader_workers=16\" \\\n",
    "                        f\"  --dataset_path={dataset_path}\" \\\n",
    "                        f\"  --dataset_format={dataset_format}\" \\\n",
    "                         \"  --model_compute_time=0\" \\\n",
    "                         \"  --ray_workers=2\" \\\n",
    "                         \"  --ray_cpus_per_worker=8\" \\\n",
    "                        f\"  --benchmark_name={benchmark_name}\" \\\n",
    "                         \"  --model_num_parameters=1000\" \\\n",
    "                         \"  --ckpt_steps=100\" \\\n",
    "                         \"  --ckpt_mode=disk\" \\\n",
    "                        f\"  --ckpt_path={ckpt_path}\"\n",
    "    \n",
    "    \n",
    "    ### --------\n",
    "    ### STEP #2: Define the runtime environment parameters for Ray workers\n",
    "    ### -------\n",
    "    \n",
    "    # Nothing to do! We use the same runtime environment definition as for the first benchmark.\n",
    "    \n",
    "    ### --------\n",
    "    ### STEP #3: Submit job to Ray cluster\n",
    "    ### -------\n",
    "    \n",
    "    job_id = ray_client.submit_job(entrypoint=entrypoint_command, runtime_env=runtime_environment)\n",
    "    \n",
    "    \n",
    "    ### Print out the Ray Job ID and other details\n",
    "    print(f\"Submitted a new Ray job with ID '{job_id}' and the following entrypoint command: \\n\")\n",
    "    for line in entrypoint_command.split('  '):\n",
    "        print(line)\n",
    "    print('^'*40, '\\n')\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Keep track of our benchmarks\n",
    "    ckpt_benchmarks[job_id] = {'name': benchmark_name, 'tag': ckpt_destination}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa4560-0a45-4a2a-ac2a-12888185b552",
   "metadata": {},
   "source": [
    "### Go to the Ray dashboard and Grafana to observe the TWO jobs running, and complete in Succeed state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a465e9-7b85-48e5-a947-1e8d1086237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Grafana Dashboard: http://{os.getenv('GRAFANA_NLB_DNS')}\")\n",
    "print(f\"Ray Dashboard: http://{os.getenv('RAY_DASHBOARD_NLB_DNS')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52f9532-a905-4afc-b840-8ef63de87bb8",
   "metadata": {},
   "source": [
    "## 7.4 Analyze and plot results (model checkpointing)\n",
    "\n",
    "Now plot the results of your checkpointing benchmarks.\n",
    "\n",
    "> ⚠️ _**NOTE:** The **two** Ray jobs that we have just submnitted will run in parallel, and the longest job will take <font color='red'>**around 4 minutes**</font> to complete. The cell below will automatically wait for the job to complete, and then plot the benchmark results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74f6a5-22fd-450c-895d-b9d8f206ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the job to finish, so that we can plot the results\n",
    "for job_id in ckpt_benchmarks:\n",
    "    utils.wait_for_job_to_finish(job_id, ray_client)\n",
    "\n",
    "# Load the log files and plot the results\n",
    "benchmark_files = {\n",
    "    benchmark['tag'].replace('_', ' ').title(): os.path.join(local_mountpoint_dir, 'logs', benchmark['name'] + '.json')\n",
    "    for benchmark in ckpt_benchmarks.values()\n",
    "}\n",
    "\n",
    "print(f\"Plotting results for '{json.dumps(benchmark_files, indent=2)}'..\")\n",
    "utils.plot_checkpointing_results(benchmark_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa992963-b3a7-4379-b0e9-647ca1632c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Explanation\n",
    "\n",
    "The benchmark results above illustrate the time required to save periodic model checkpoints, either directly to S3 using Mountpoint for S3 (in red) or to a 'local' EBS volume (in blue). The results indicate a clear advantage when saving checkpoints directly to S3, as checkpointing times are consistently lower compared to saving to local storage.\n",
    "\n",
    "This higher throughput, close to the maximum network bandwidth of the EC2 instance, provides significant time and cost savings during training. Particularly for long-running distributed training jobs with frequent checkpointing requirements, reducing overall training overhead and allowing more efficient resource utilization.\n",
    "\n",
    ">💡 _**TIP:** In this workshop, we have used EBS storage. If you use EC2 instances with instance store you will also have one or more physically attached ephemeral volumes. Instance store is ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4a63c1-65dd-4e3a-bd22-a6f6484ee13a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 8. Summary\n",
    "\n",
    "<a id='sec-8'></a>\n",
    "\n",
    "This workshop demonstrated practical techniques for optimizing data I/O for AI/ML workloads on AWS using Mountpoint for S3. Through hands-on experiments and benchmarks, you explored key aspects of ML infrastructure optimization. Let's recap what you have done in the workshop and summarize the identified best practices.\n",
    "\n",
    "#### Shared storage\n",
    "- After learning about the dataset on your local storage, you used the _AWS CLI_ and _s5cmd_, an open source tool, to migrate this dataset to Amazon S3;\n",
    "- You installed Mountpoint for Amazon S3, and learned how you can use this interact with data in S3 as if it were a local file system.\n",
    "\n",
    "#### Infrastructure integration\n",
    "- You used the integration between Ray clusters on Amazon EKS and S3 storage using the [Mountpoint for Amazon S3 Container Storage Interface (CSI) Driver](https://github.com/awslabs/mountpoint-s3-csi-driver), which allows Kubernetes applications to access Amazon S3 objects through a file system interface;\n",
    "- You also learned how to scale ML workloads across multiple compute nodes with Ray while maintaining efficient data access.\n",
    "\n",
    "#### Efficient data loading\n",
    "- We compared two approaches for organizing training data in S3: individual files vs sharded datasets, demonstrating that sharding samples into larger files (using, e.g., TAR- or TFRecord-formats) significantly improved data loading performance;\n",
    "- The improvement stems from reduced number of S3 API calls and associated TTFB latency overhead when reading dataset samples sequentially from S3;\n",
    "- In cases when sharding is not possible, we demostrated that local caching is another robust strategy to incorporate, especially when running multiple epochs.\n",
    "\n",
    "#### Model checkpointing\n",
    "- Model checkpointing directly to S3 using Mountpoint for Amazon S3 is potentially significantly faster than checkpointing to local storage;\n",
    "- This efficiency comes from Mountpoint for Amazon S3's ability to distribute writes across the S3 fleet;\n",
    "- Additionally, this approach eliminates the traditional two-step process of saving to local storage followed by cloud storage syncing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c945e-aa15-4972-9c04-6c4e39b0ebb9",
   "metadata": {},
   "source": [
    "\n",
    "# 9. Next steps\n",
    "\n",
    "<a id='sec-9'></a>\n",
    "\n",
    "- If you have time now, and wish to learn about the **Amazon S3 Connector for PyTorch**, <font color='blue'>**_please proceed to the Bonus Notebook_**</font>  by opening `2_bonus_notebook.ipynb` in the JupyterLab file browser panel (on the left).\n",
    "S3 Connector for PyTorch provides an even tighter integration between ML raining workloads with PyTorch and the S3 storage service, offering both high-performance dataloading and model checkpointing.\n",
    "\n",
    "- If you wish to share this workshop with colleagues, and/or run it in your own time in your own AWS Account, make a note of this link: https://s12d.com/stg406\n",
    "\n",
    "- If you are at an AWS event, **please fill out the session survey provided by AWS staff**. Your feedback helps us improve, and justifies our efforts in creating content such as this. Thank you.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
